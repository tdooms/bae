{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195b387",
   "metadata": {},
   "source": [
    "# Manifolds\n",
    "This notebook discusses how to find manifolds using bilinear autoencoders.\n",
    "\n",
    "> We assume the reader is familiar with the following:\n",
    "> - Bilinear autoencoder basics\n",
    "> - Toy models of superposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d304332",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup\n",
    "Let's get the setup out of the way.\n",
    "As always, you can find a ``pyproject.toml`` file in the repo that works with the [uv](https://docs.astral.sh/uv/guides/install-python/) package manager.\n",
    "\n",
    "// TODO: make this automatic \\\n",
    "You can download the autoencoders from [here](https://drive.google.com/drive/folders/1Qm8tSu0pi08lGAvqvW6YtddYzro-Cudc).\n",
    "I personally looked at 'the autoencoders with '2' and '10' and the end of their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87162bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from einops import einsum\n",
    "\n",
    "from utils.feature import Feature\n",
    "from utils.manifold import Manifold\n",
    "from utils.functions import *\n",
    "from autoencoder import Autoencoder\n",
    "\n",
    "import plotly.express as px\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11474dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "name = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "# Download the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForCausalLM.from_pretrained(name, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "tokenize = lambda dataset: tokenizer(dataset[\"text\"], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Download the dataset and tokenize it\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming=True).with_format(\"torch\")\n",
    "dataset = Dataset.from_list(list(dataset.take(2**11))).with_format(\"torch\")\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e144b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the autoencoder\n",
    "coder = Autoencoder.load(model, layer=18, expansion=16, alpha=0.2).half()\n",
    "\n",
    "# Load in the feature max-activation visualizer, this can take a while (reduce batch size if needed)\n",
    "vis = Feature(coder, tokenizer, dataset, max_steps=2**4, batch_size=2**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679302f",
   "metadata": {},
   "source": [
    "## Finding dependence through toy models of superposition\n",
    "\n",
    "Now, onto the good stuff. \n",
    "\n",
    "The [TMS paper](https://transformer-circuits.pub/2022/toy_model/index.html) studies how models represent sparse features when they are tasked to reconstruct through a bottleneck. This is a form of autoencoder, albeit a boring linear one. In short, the papers show that models naturally create interesting geometries based on correlation or anti-correlation.\n",
    "\n",
    "Generally, speaking the results from TMS are interesting but their experiments can't be scaled to larger models.\n",
    "Until now.\n",
    "Bilinear autoencoders actually follow the same setup since its encoder and decoder are each other's transpose.\n",
    "Yet, they're nonlinear in their inputs, which makes it a cool tool to study non-linear dynamics in a tractable manner.\n",
    "\n",
    "---\n",
    "\n",
    "We can conceptually split a bilinear autoencoder into two parts:\n",
    "- A bilinear part: the $L$ and $R$ matrix, along with the element-wise product.\n",
    "- A linear part: the $D$ matrix, which takes the feature basis and projects further down.\n",
    "\n",
    "Here, we will deep dive into why this linear bit is so interesting.\n",
    "In short, this linear bottleneck 'clusters' quadratic features, which correspond to higher-dimensional manifolds.\n",
    "\n",
    "## What kind of manifolds can we find?\n",
    "Each feature in a bilinear autoencoder describes a non-linear manifold on their own. \n",
    "Unfortunately, these are generally quite boring and basically just correspond to linear directions with a XOR (TODO: explain why).\n",
    "However, they can become interesting when composed.\n",
    "Then, they describe general [quadrics](https://www.wikiwand.com/en/articles/Quadric), think circles and other higher-dimensional conic sections.\n",
    "\n",
    "> The features don't actually describe a manifold, they just assign a value (non-linearly) to the whole input space.\n",
    "> they do describe an actual manifold when we threshold this value. For instance, the space where $f(x) > 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed433ea8",
   "metadata": {},
   "source": [
    "## How does this work in practice?\n",
    "\n",
    "With the intuition out of the way let's look at what this yields.\n",
    "First, we need to have a measure of which features get clustered through the $D$ matrix. \n",
    "\n",
    "> I won't go into too much details here, read TMS for more intuition\n",
    "\n",
    "We compute the effective dimension; a continuous measure of the amount of 'big' numbers in a vector. \n",
    "This roughly corresponds to finding the number of other features with which it interacts in the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f22a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the (normalized) grammian matrix\n",
    "d = coder.down / coder.down.norm(dim=0, keepdim=True)\n",
    "g = d.T @ d\n",
    "\n",
    "# Compute and plot the effective dimension (somtimes called participation ratio) per feature.\n",
    "gpr = generalized_effective_dimension(g)\n",
    "fig = px.scatter(y=gpr.cpu(), x=list(range(gpr.size(-1))), template='plotly_white', width=600, height=300, title=\"Number of active elements in the overlap matrix\")\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=30, b=0), showlegend=False).show()\n",
    "\n",
    "# I recommend not looking at the say top 5/10-ish. \n",
    "# There's some dense features which I don't quite understand yet.\n",
    "# Their manifolds are interesting though.\n",
    "\n",
    "# Print the top 50 features with the highest effective dimension, possibly corresponding to interesting manifolds.\n",
    "print(gpr.topk(50).indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57849e4f",
   "metadata": {},
   "source": [
    "We see that most features are roughly 1-dimensional. \n",
    "This means they gently interact with other features but probably don't have much additional structure or overlap.\n",
    "\n",
    "---\n",
    "\n",
    "Okay, let's take a look at some of these points. \n",
    "I have selected a few that were interesting to me.\n",
    "\n",
    "We then show the features and their max activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are for the coder with '10' tag\n",
    "# idx = 602\n",
    "# idx = 11023\n",
    "# idx = 7695\n",
    "\n",
    "# These are for the coder with '2' tag\n",
    "# idx = 10313  # abbreviation manifold\n",
    "# idx = 3338 # ( manifold\n",
    "# idx = 15620 # numbers!\n",
    "# idx = 36 # of\n",
    "idx = 2062\n",
    "\n",
    "# Plot the overlaps of the selected feaature\n",
    "fig = px.histogram(g[idx].cpu(), template='plotly_white', log_y=True, width=600, height=300, range_x=[-1.1, 1.1])\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=30, b=0), showlegend=False).show()\n",
    "\n",
    "# Visualise the top 5 features\n",
    "inds = g[idx].abs().topk(k=5).indices\n",
    "vis(inds.tolist(), k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7fb8d",
   "metadata": {},
   "source": [
    "Given these features, we can construct a metric tensor (also known as a density matrix) that describes the manifold.\n",
    "We can then decompose this matrix using an eigendecomposition to show how many dimensional the manifold actually is.\n",
    "\n",
    "You'll see that sometimes, even when using many different features, the manifold will be roughly one dimensional.\n",
    "This can have muliple interpretations but one of them is that the original features were 'split' for sparsity reasons for instance.\n",
    "Another is that the feature was simply a 'building block' and is used across other reconstructions in some way.\n",
    "Luckily, we can just analyse the composition of this feature as if it were one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "density = einsum(coder.left[inds], coder.right[inds], \"out in1, out in2 -> in1 in2\")\n",
    "density = 0.5 * (density + density.T)\n",
    "\n",
    "manifold = Manifold(dataset, coder.hooked, tokenizer, density, max_steps=2**5)\n",
    "manifold.spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dd958",
   "metadata": {},
   "source": [
    "Then, finally, we can sample a bunch of inputs (handled efficiently by the manifold class) and plot them.\n",
    "\n",
    "There are some important thing to note here. The most significant being how these are exactly visualised.\n",
    "The dimensionality reduction happens linearly but not through PCA. \n",
    "Rather, we use the autoencoder itself to compute the principal dimensions of the manifold, independently of inputs.\n",
    "This is quite important as many methods until now were reliant on sampling few points (such that the principal axes correspond to the things we actually want). \n",
    "But this could lead to some illusorry conclusions. \n",
    "We want our visualisations to align with the manifold, not what we propagated through the network to sample it.\n",
    "\n",
    "Our approach simply uses the autoencoder to find a linear projection which likely contains a manifold, then we just project the inputs into it.\n",
    "\n",
    "Finally, color correspond to the activation of the metric tensor $xMx$. \n",
    "Hovering over a sample shows the current token and the top model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac60a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the manifold, increase `k` to show more points or decrease `total` to sample fewer points.\n",
    "manifold(k=40_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
